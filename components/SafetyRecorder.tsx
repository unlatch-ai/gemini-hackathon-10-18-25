import React, { useState, useRef, useEffect } from 'react';

interface SafetyRecorderProps {
  onCodewordDetected?: () => void;
}

interface AgentScores {
  transcript: number;
  emotional: number;
  context: number;
  final: number;
}

const SafetyRecorder: React.FC<SafetyRecorderProps> = ({ onCodewordDetected }) => {
  const [isRecording, setIsRecording] = useState(false);
  const [status, setStatus] = useState('idle');
  const [recordingTime, setRecordingTime] = useState(0);
  const [dangerDetected, setDangerDetected] = useState(false);
  const [callTriggered, setCallTriggered] = useState(false);
  const [transcripts, setTranscripts] = useState<string[]>([]);
  const [dangerScore, setDangerScore] = useState<number | null>(null);
  const [agentScores, setAgentScores] = useState<AgentScores | null>(null);
  const [analyzing, setAnalyzing] = useState(false);

  const wsRef = useRef<WebSocket | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  const videoRef = useRef<HTMLVideoElement | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const recognitionRef = useRef<any>(null);

  useEffect(() => {
    return () => {
      // Cleanup on unmount
      if (wsRef.current) {
        wsRef.current.close();
      }
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
        mediaRecorderRef.current.stop();
      }
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
    };
  }, []);

  const startRecording = async () => {
    try {
      setStatus('connecting');

      // Connect to WebSocket (works with ngrok via Vite proxy)
      const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
      const host = window.location.host; // includes port
      const wsUrl = import.meta.env.VITE_WS_URL || `${protocol}//${host}/ws/live-session`;

      console.log('Connecting to WebSocket:', wsUrl);
      const ws = new WebSocket(wsUrl);
      wsRef.current = ws;

      ws.onopen = async () => {
        console.log('âœ… WebSocket connected');
        setStatus('connected');

        // Start video + audio recording
        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            audio: {
              channelCount: 1,
              sampleRate: 16000,
              echoCancellation: true,
              noiseSuppression: true,
            },
            video: {
              facingMode: 'user', // Use 'environment' for rear camera
              width: { ideal: 1280 },
              height: { ideal: 720 }
            }
          });

          streamRef.current = stream;

          // Display video feed
          if (videoRef.current) {
            videoRef.current.srcObject = stream;
          }

          // Create audio-only MediaRecorder for Gemini (simpler than extracting from video)
          const audioStream = new MediaStream(stream.getAudioTracks());
          const mediaRecorder = new MediaRecorder(audioStream, {
            mimeType: 'audio/webm;codecs=opus'
          });
          mediaRecorderRef.current = mediaRecorder;

          // Send audio chunks to backend
          mediaRecorder.ondataavailable = async (event) => {
            if (event.data.size > 0 && ws.readyState === WebSocket.OPEN) {
              // Convert to base64
              const reader = new FileReader();
              reader.onloadend = () => {
                const base64 = (reader.result as string).split(',')[1];
                ws.send(JSON.stringify({
                  type: 'audio_chunk',
                  audio: base64
                }));
              };
              reader.readAsDataURL(event.data);
            }
          };

          mediaRecorder.start(1000); // Send chunks every second
          setIsRecording(true);
          setStatus('recording');

          // Start timer
          timerRef.current = setInterval(() => {
            setRecordingTime(prev => prev + 1);
          }, 1000);

          // Send start recording message
          ws.send(JSON.stringify({ type: 'start_recording' }));

          // Start Web Speech API for continuous speech recognition
          if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
            const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition;
            const recognition = new SpeechRecognition();
            recognition.continuous = true;
            recognition.interimResults = false;
            recognition.lang = 'en-US';

            recognition.onresult = (event: any) => {
              const transcript = event.results[event.results.length - 1][0].transcript.trim();
              console.log('ğŸ™ï¸  Heard:', transcript);

              // Add to transcript display
              setTranscripts(prev => [...prev.slice(-10), transcript]); // Keep last 10

              // Send to backend
              if (ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({
                  type: 'trigger_codeword',
                  text: transcript
                }));
              }
            };

            recognition.onerror = (event: any) => {
              console.error('Speech recognition error:', event.error);
            };

            recognition.start();
            recognitionRef.current = recognition;
            console.log('âœ… Speech recognition started');
          } else {
            console.warn('Speech recognition not supported - use button instead');
          }

        } catch (error) {
          console.error('Error accessing camera/microphone:', error);
          setStatus('error');
          alert('Could not access camera/microphone. Please allow permissions.');
        }
      };

      ws.onmessage = (event) => {
        const data = JSON.parse(event.data);
        console.log('ğŸ“¨ WebSocket message:', data);

        switch (data.type) {
          case 'connected':
            console.log('Session ID:', data.sessionId);
            break;

          case 'recording_started':
            console.log('âœ… Recording started on server');
            break;

          case 'analysis_started':
            console.log('ğŸ¤– Multi-agent analysis started...');
            setAnalyzing(true);
            break;

          case 'analysis_complete':
            console.log('ğŸ“Š Multi-agent analysis complete:', data);
            setAnalyzing(false);
            if (data.agentScores) {
              setAgentScores(data.agentScores);
            }
            // Don't set danger detected unless score >= 70
            if (data.dangerScore < 70) {
              setDangerDetected(false);
            }
            break;

          case 'codeword_detected':
            console.log('ğŸš¨ DANGER DETECTED!', data);
            setAnalyzing(false);
            setDangerDetected(true);
            setDangerScore(data.confidence ? Math.round(data.confidence * 100) : null);
            // Extract agent scores if available
            if (data.agentScores) {
              setAgentScores(data.agentScores);
            }
            setStatus('danger_detected');
            if (onCodewordDetected) {
              onCodewordDetected();
            }
            break;

          case 'call_triggered':
            console.log('ğŸ“ Call triggered!', data);
            setCallTriggered(true);
            setStatus('call_triggered');
            break;

          case 'call_failed':
            console.error('âŒ Call failed:', data.error);
            alert(`Failed to trigger call: ${data.error}`);
            break;

          case 'error':
            console.error('âŒ Error:', data.message);
            setStatus('error');
            break;
        }
      };

      ws.onerror = (error) => {
        console.error('âŒ WebSocket error:', error);
        setStatus('error');
      };

      ws.onclose = () => {
        console.log('WebSocket closed');
        setStatus('disconnected');
      };

    } catch (error) {
      console.error('Error starting recording:', error);
      setStatus('error');
      alert('Failed to start recording');
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
      mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
    }

    if (recognitionRef.current) {
      recognitionRef.current.stop();
      recognitionRef.current = null;
    }

    if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
      wsRef.current.send(JSON.stringify({ type: 'stop_recording' }));
      wsRef.current.close();
    }

    if (timerRef.current) {
      clearInterval(timerRef.current);
    }

    setIsRecording(false);
    setStatus('stopped');
  };

  const testDangerPhrase = async () => {
    if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
      console.log('ğŸ§ª Testing multi-agent danger detection...');
      wsRef.current.send(JSON.stringify({
        type: 'trigger_codeword',
        text: "I'm feeling really unsafe and uncomfortable right now. I want to leave but I can't."
      }));
    }
  };

  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
  };

  const getStatusColor = () => {
    if (callTriggered) return 'bg-purple-600';
    if (dangerDetected) return 'bg-red-600 animate-pulse';
    if (isRecording) return 'bg-green-600 animate-pulse';
    return 'bg-gray-600';
  };

  const getStatusText = () => {
    if (callTriggered) return 'ğŸ“ Emergency Call Triggered';
    if (dangerDetected) return `ğŸš¨ Dangerous Situation Detected! (${dangerScore}%)`;
    if (status === 'recording') return 'ğŸ‘‚ AI Monitoring Active...';
    if (status === 'connecting') return 'Connecting...';
    if (status === 'connected') return 'Connected';
    if (status === 'error') return 'âŒ Error';
    return 'Not Recording';
  };

  return (
    <div className="bg-gray-800 rounded-lg shadow-lg p-6 max-w-2xl mx-auto">
      <h2 className="text-2xl font-bold text-white mb-6 text-center">
        ğŸ›¡ï¸ Safety Monitor
      </h2>

      {/* Video Feed */}
      <div className="mb-6 relative bg-black rounded-lg overflow-hidden" style={{ aspectRatio: '16/9' }}>
        <video
          ref={videoRef}
          autoPlay
          playsInline
          muted
          className="w-full h-full object-cover"
        />
        {!isRecording && (
          <div className="absolute inset-0 flex items-center justify-center bg-gray-900/80">
            <p className="text-gray-400 text-lg">ğŸ“¹ Camera feed will appear here</p>
          </div>
        )}
        {isRecording && (
          <div className="absolute top-4 left-4 bg-red-600 text-white px-3 py-1 rounded-full text-sm font-semibold flex items-center gap-2">
            <span className="w-2 h-2 bg-white rounded-full animate-pulse"></span>
            REC
          </div>
        )}
      </div>

      {/* Status Indicator */}
      <div className="mb-6">
        <div className={`${getStatusColor()} text-white text-center py-3 px-4 rounded-lg font-semibold`}>
          {getStatusText()}
        </div>
      </div>

      {/* Recording Time */}
      {isRecording && (
        <div className="text-center mb-6">
          <div className="text-5xl font-mono text-white">
            {formatTime(recordingTime)}
          </div>
          <div className="text-sm text-gray-400 mt-2">Recording Duration</div>
        </div>
      )}

      {/* Live Transcript Display */}
      {isRecording && transcripts.length > 0 && (
        <div className="mb-4 bg-gray-900/50 border border-gray-700 rounded-lg p-4 max-h-32 overflow-y-auto">
          <p className="text-xs text-gray-400 mb-2">Live Transcript (Multi-Agent AI Analysis Every 10s):</p>
          {transcripts.map((text, idx) => (
            <p key={idx} className="text-sm text-gray-300 mb-1">
              â€¢ {text}
            </p>
          ))}
        </div>
      )}

      {/* Analyzing Indicator */}
      {analyzing && (
        <div className="mb-4 bg-blue-900/30 border border-blue-500 rounded-lg p-4 animate-pulse">
          <p className="text-center text-blue-300 font-semibold">
            ğŸ¤– 4 Gemini Agents Analyzing...
          </p>
          <p className="text-center text-xs text-blue-400 mt-1">
            Transcript â€¢ Emotional â€¢ Context â€¢ Threat Assessment
          </p>
        </div>
      )}

      {/* Multi-Agent Analysis Display */}
      {agentScores && !analyzing && (
        <div className="mb-4 bg-gray-900/50 border border-blue-500 rounded-lg p-4">
          <p className="text-sm text-blue-300 font-semibold mb-3">ğŸ¤ Multi-Agent Analysis Results:</p>
          <div className="grid grid-cols-2 gap-3">
            <div className="bg-gray-800 p-3 rounded">
              <div className="text-xs text-gray-400">ğŸ“ Transcript Agent</div>
              <div className="text-lg font-bold text-white">{agentScores.transcript}%</div>
              <div className="text-xs text-gray-500">Literal content analysis</div>
            </div>
            <div className="bg-gray-800 p-3 rounded">
              <div className="text-xs text-gray-400">ğŸ˜° Emotional Agent</div>
              <div className="text-lg font-bold text-white">{agentScores.emotional}%</div>
              <div className="text-xs text-gray-500">Emotional distress detection</div>
            </div>
            <div className="bg-gray-800 p-3 rounded">
              <div className="text-xs text-gray-400">ğŸ” Context Agent</div>
              <div className="text-lg font-bold text-white">{agentScores.context}%</div>
              <div className="text-xs text-gray-500">Situational dynamics</div>
            </div>
            <div className="bg-gradient-to-br from-red-900 to-orange-900 p-3 rounded border-2 border-red-500">
              <div className="text-xs text-gray-200">âš–ï¸  Threat Assessor</div>
              <div className="text-2xl font-bold text-white">{agentScores.final}%</div>
              <div className="text-xs text-gray-300">Final consensus score</div>
            </div>
          </div>
        </div>
      )}

      {/* Alert Messages */}
      {dangerDetected && !callTriggered && (
        <div className="bg-red-500/20 border border-red-500 rounded-lg p-4 mb-4">
          <p className="text-red-300 text-center font-semibold">
            ğŸš¨ Dangerous situation detected by AI! (Score: {dangerScore}%)
          </p>
          <p className="text-red-200 text-sm text-center mt-1">
            Triggering emergency call...
          </p>
        </div>
      )}

      {callTriggered && (
        <div className="bg-purple-500/20 border border-purple-500 rounded-lg p-4 mb-4">
          <p className="text-purple-300 text-center font-semibold">
            ğŸ“ Emergency call initiated!
          </p>
          <p className="text-purple-200 text-sm text-center mt-2">
            Answer your phone to receive the fake call scenario.
          </p>
        </div>
      )}

      {/* Control Buttons */}
      <div className="space-y-3">
        {!isRecording ? (
          <button
            onClick={startRecording}
            disabled={status === 'connecting'}
            className="w-full bg-green-600 hover:bg-green-700 disabled:bg-gray-600 text-white font-bold py-4 px-6 rounded-lg transition-colors text-lg"
          >
            {status === 'connecting' ? 'Connecting...' : 'ğŸ™ï¸ Start Safety Recording'}
          </button>
        ) : (
          <>
            <button
              onClick={testDangerPhrase}
              className="w-full bg-orange-600 hover:bg-orange-700 text-white font-bold py-4 px-6 rounded-lg transition-colors text-lg"
            >
              ğŸ§ª Test Multi-Agent Detection
            </button>
            <button
              onClick={stopRecording}
              className="w-full bg-red-600 hover:bg-red-700 text-white font-bold py-4 px-6 rounded-lg transition-colors text-lg"
            >
              â¹ï¸ Stop Recording
            </button>
          </>
        )}
      </div>

      {/* Instructions */}
      <div className="mt-6 p-4 bg-blue-500/10 border border-blue-500/30 rounded-lg">
        <p className="text-sm text-blue-300">
          <strong>How it works:</strong>
        </p>
        <ol className="text-sm text-blue-200 mt-2 space-y-1 list-decimal list-inside">
          <li>Click "Start Safety Recording"</li>
          <li>4 specialized Gemini AI agents analyze your conversation every 10 seconds</li>
          <li>If dangerous/uncomfortable situation detected (70%+ confidence)</li>
          <li>You'll automatically receive a fake phone call</li>
          <li>Use the call as an excuse to leave safely</li>
        </ol>
        <p className="text-xs text-gray-400 mt-3">
          Powered by Gemini 2.0 Flash Multi-Agent System
        </p>
      </div>
    </div>
  );
};

export default SafetyRecorder;
